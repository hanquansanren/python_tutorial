{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40., grad_fn=<MeanBackward0>)\n",
      "#########反向传播########\n",
      "None\n",
      "tensor(28.) tensor(8.) tensor(10.)\n",
      "None None None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\anaconda\\envs\\gpt\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "input = torch.ones([2, 2], requires_grad=False) # [1.0, 1.0, 1.0, 1.0]\n",
    "w1 = torch.tensor(2.0, requires_grad=True)      # [2.0, 2.0, 2.0, 2.0]\n",
    "w2 = torch.tensor(3.0, requires_grad=True)      # [3.0, 3.0, 3.0, 3.0]\n",
    "w3 = torch.tensor(4.0, requires_grad=True)      # [4.0, 4.0, 4.0, 4.0]\n",
    "\n",
    "l1 = input * w1  # l1 = input x w1 = [2.0, 2.0, 2.0, 2.0]\n",
    "l2 = l1 + w2     # l2 = l1 + w2 = [5.0, 5.0, 5.0, 5.0]\n",
    "l3 = l1 * w3     # l3 = l1 x w3 = [8.0, 8.0, 8.0, 8.0] \n",
    "l4 = l2 * l3     # l4 = l2 x l3 = [40.0, 40.0, 40.0, 40.0] \n",
    "loss = l4.mean() # loss = mean(l4) = 40.0   loss.grad\n",
    "print(loss)\n",
    "\n",
    "\n",
    "\n",
    "# print(w1.data, w1.grad, w1.grad_fn)\n",
    "# # tensor(2.) None None\n",
    "\n",
    "# print(l1.data, l1.grad, l1.grad_fn)\n",
    "# # tensor([[2., 2.],\n",
    "# #         [2., 2.]]) None <MulBackward0 object at 0x000001EBE79E6AC8>\n",
    "\n",
    "# print(loss.data, loss.grad, loss.grad_fn)\n",
    "# # tensor(40.) None <MeanBackward0 object at 0x000001EBE79D8208>\n",
    "\n",
    "print(\"#########反向传播########\")\n",
    "loss.backward()\n",
    "\n",
    "print(w1.grad, w2.grad, w3.grad)\n",
    "# tensor(28.) tensor(8.) tensor(10.)\n",
    "print(l1.grad, l2.grad, l3.grad, l4.grad, loss.grad) # 非叶张量，因此为None\n",
    "# None None None None None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a是否是叶张量 True\n",
      "b是否是叶张量 False\n",
      "b是否需要计算梯度 True\n",
      "None tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "<AddBackward0 object at 0x000002260C8826D0> None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\anaconda\\envs\\gpt\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# 尝试获取非叶张量b的梯度值，结果为None\n",
    "a = torch.ones([2, 2], requires_grad=True)\n",
    "# a = torch.ones([2, 2], requires_grad=False)\n",
    "print(\"a是否是叶张量\",a.is_leaf) # 只要是用户创建的张量，均为叶张量，与是否requires_grad无关\n",
    "\n",
    "\n",
    "b = 3*a + 2 # b是a的深拷贝，不共享内存，但共享计算图（保留梯度传播关系）。\n",
    "loss=b.sum()\n",
    "print(\"b是否是叶张量\",b.is_leaf)\n",
    "print(\"b是否需要计算梯度\",b.requires_grad)\n",
    "loss.backward()\n",
    "\n",
    "print(a.grad_fn, a.grad)\n",
    "print(b.grad_fn, b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a是否是叶张量 True\n",
      "b是否是叶张量 False\n",
      "b是否需要计算梯度 True\n",
      "None tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "<AddBackward0 object at 0x000002260C8AD5B0> tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 尝试获取非叶张量b的梯度值，但采用了保留梯度的设置，可以获得结果\n",
    "a = torch.ones([2, 2], requires_grad=True)\n",
    "# a = torch.ones([2, 2], requires_grad=False)\n",
    "print(\"a是否是叶张量\",a.is_leaf) # 只要是用户创建的张量，均为叶张量，与是否requires_grad无关\n",
    "\n",
    "\n",
    "b = 3*a + 2 # b是a的深拷贝，不共享内存，但共享计算图（保留梯度传播关系）。\n",
    "b.retain_grad() # 务必写在反向传播之前！\n",
    "loss=b.sum()\n",
    "print(\"b是否是叶张量\",b.is_leaf)\n",
    "print(\"b是否需要计算梯度\",b.requires_grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "print(a.grad_fn, a.grad)\n",
    "print(b.grad_fn, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inplace 操作,不更改内存地址，就能实现操作，有利于节省内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2362253296896\n",
      "2362253305216\n",
      "2362253296896\n",
      "tensor([7.3891, 2.7183])\n",
      "2362253303616\n",
      "2362253303616\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2,1])\n",
    "print(a.storage().data_ptr()) # 2362253296896\n",
    "print(a.exp().storage().data_ptr()) # 2362253305216\n",
    "print(a.storage().data_ptr()) # 2362253296896\n",
    "\n",
    "# 情景 1\n",
    "a = a.exp() # 非in-place操作\n",
    "print(a) # tensor([7.3891, 2.7183])\n",
    "\n",
    "\n",
    "print(a.storage().data_ptr()) # 2362253303616\n",
    "# 情景 2\n",
    "a[0] = 10 # in-place操作\n",
    "print(a.storage().data_ptr()) # 2362253303616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "b是不是叶张量 False\n",
      "b是不是需要梯度 True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 对非叶张量，在backward之前进行就地操作。会报错\n",
    "a = torch.tensor([1.0, 3.0], requires_grad=True)\n",
    "b = a + 2 # b是一个非叶张量。允许在使用前进行就地操作\n",
    "print(b._version) # 0\n",
    "print(\"b是不是叶张量\",b.is_leaf)\n",
    "print(\"b是不是需要梯度\",b.requires_grad)\n",
    "b[0] = 1000.0 # 在使用之前进行，一次就地\n",
    "b[0] = 1 # 在使用之前进行，二次就地\n",
    "loss = (b * b).mean()\n",
    "# b[0] = 1000.0\n",
    "print(b._version) # 1\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "a是不是叶张量 True\n",
      "a是不是需要梯度 True\n",
      "tensor([2., 4.], grad_fn=<AddBackward0>) False\n",
      "0\n",
      "0\n",
      "b是不是叶张量 False\n",
      "b是不是需要梯度 True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 对叶张量，任何时候不允许进行就地操作。会报错\n",
    "a = torch.tensor([1.0, 3.0], requires_grad=True)\n",
    "print(a._version) # 0\n",
    "print(\"a是不是叶张量\",a.is_leaf)\n",
    "print(\"a是不是需要梯度\",a.requires_grad)\n",
    "# a += 1 # 就地操作是非法的\n",
    "a = a+1 # 非就地操作是允许的\n",
    "print(a, a.is_leaf) # 变成了非叶张量\n",
    "print(a._version) # 0\n",
    "\n",
    "b = a + 2 # b是一个非叶张量。允许在使用前进行就地操作\n",
    "print(b._version) # 0\n",
    "print(\"b是不是叶张量\",b.is_leaf)\n",
    "print(\"b是不是需要梯度\",b.requires_grad)\n",
    "b[0] = 1000.0 # 在使用之前进行，一次就地\n",
    "b[0] = 1 # 在使用之前进行，二次就地\n",
    "loss = (b * b).mean()\n",
    "print(b._version) # 1\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  5.,  2.,  3.], requires_grad=True) True 2362444863024 2362253303680 True\n",
      "tensor([10., 10., 10., 10.], requires_grad=True) True 2362444863024 2362253303680 True\n",
      "tensor([10., 10., 10., 10.], requires_grad=True) True 2362444863024 True\n",
      "tensor(100., grad_fn=<MeanBackward0>)\n",
      "tensor([5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# 走后门（从内存直接修改法）改变叶张量的方法\n",
    "# 方法一\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "print(a, a.is_leaf, id(a),a.storage().data_ptr(),a.requires_grad)\n",
    "\n",
    "a.data.fill_(10.)\n",
    "# a.detach().fill_(10.)\n",
    "# a.detach()\n",
    "print(a, a.is_leaf, id(a),a.storage().data_ptr(),a.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "loss = (a*a).mean()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(a.grad)\n",
    "# tensor([5., 5., 5., 5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  5.,  2.,  3.], requires_grad=True) True 2362445062448 2362253303872 True\n",
      "tensor([10., 10., 10., 10.], requires_grad=True) True 2362445062448 2362253303872 True\n",
      "tensor([5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# 走后门的方法二：只能用于使用前，一般情况下，是用作推理的\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "print(a, a.is_leaf, id(a),a.storage().data_ptr(),a.requires_grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    a[:] = 10.\n",
    "print(a, a.is_leaf, id(a),a.storage().data_ptr(),a.requires_grad)\n",
    "\n",
    "\n",
    "loss = (a*a).mean()\n",
    "\n",
    "# 放在这里会in-place报错\n",
    "# with torch.no_grad():\n",
    "#     a[:] = 10.\n",
    "# print(a, a.is_leaf, id(a),a.storage().data_ptr(),a.requires_grad)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "print(a.grad)\n",
    "# tensor([5., 5., 5., 5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True) True 2362443811072 2362253308032 True\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>) False 2362447369856 2362253309568 True\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>) False 2362447369856 2362253309568 True\n",
      "tensor([0.7311, 0.8808, 0.9526]) True 2362445409232 2362253308864 False\n",
      "out就地操作后\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>) False 2362447369856 2362253309568 True\n",
      "c就地操作后\n",
      "tensor([10.0000,  0.8808,  0.9526], requires_grad=True) True 2362445409232 2362253308864 True\n",
      "0\n",
      "tensor([0.3932, 0.2100, 0.0904])\n",
      "tensor([0.0000, 0.0301, 0.0196]) None\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 知乎网友提问：\n",
    "a = torch.tensor([1.,2.,3.], requires_grad=True)\n",
    "print(a, a.is_leaf, id(a),a.storage().data_ptr(),a.requires_grad)\n",
    "out=a.sigmoid()\n",
    "print(out, out.is_leaf, id(out),out.storage().data_ptr(),out.requires_grad)\n",
    "# out=torch.sigmoid(a)\n",
    "# out=a**2\n",
    "\n",
    "c=out.detach().clone()\n",
    "print(out, out.is_leaf, id(out),out.storage().data_ptr(),out.requires_grad)\n",
    "print(c, c.is_leaf, id(c),c.storage().data_ptr(),c.requires_grad)\n",
    "c[0]=10\n",
    "c.requires_grad_() # 11和12行，若顺序调换，则报错\n",
    "print(\"out就地操作后\")\n",
    "print(out, out.is_leaf, id(out),out.storage().data_ptr(),out.requires_grad)\n",
    "print(\"c就地操作后\")\n",
    "print(c, c.is_leaf, id(c),c.storage().data_ptr(),c.requires_grad)\n",
    "\n",
    "print(out._version)\n",
    "\n",
    "loss=out*2\n",
    "loss2=torch.sigmoid(c*6)\n",
    "\n",
    "loss.sum().backward()\n",
    "loss2.sum().backward()\n",
    "print(a.grad)\n",
    "\n",
    "print(c.grad,c.grad_fn)\n",
    "print(c._version)\n",
    "print(out._version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 钩子用法\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b87ae1c415bf879c31e2b7111dedfdde99e7fb6020d8739c335659c250c99311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
