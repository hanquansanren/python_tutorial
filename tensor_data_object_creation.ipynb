{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1灵活的初始化方法\n",
    "支持list或ndarray输入，默认继承原始的数据类型，并且支持指定数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, -1],\n",
      "        [ 1, -1]])\n",
      "dtype of a is torch.int64\n",
      "id of a is 3232800068608\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "dtype of b is torch.int32\n",
      "id of b is 3232800062208\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "dtype of c is torch.int64\n",
      "id of c is 3232796620032\n",
      "\n",
      "tensor([[ 1, -1],\n",
      "        [ 1, -1]], dtype=torch.int32)\n",
      "dtype of d is torch.int32\n",
      "id of d is 3232495737088\n"
     ]
    }
   ],
   "source": [
    "# 使用 torch.tensor()初始化\n",
    "a = torch.tensor([[1, -1], [1, -1]])\n",
    "b = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "c = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int64))\n",
    "d = torch.tensor([[1, -1], [1, -1]], dtype=torch.int32)\n",
    "print(a) \n",
    "print('dtype of a is',a.dtype)\n",
    "print('id of a is',a.data_ptr(),end='\\n\\n')\n",
    "\n",
    "print(b)\n",
    "print('dtype of b is',b.dtype)\n",
    "print('id of b is',b.data_ptr(),end='\\n\\n')\n",
    "\n",
    "print(c)\n",
    "print('dtype of c is',c.dtype)\n",
    "print('id of c is',c.data_ptr(),end='\\n\\n')\n",
    "\n",
    "print(d)\n",
    "print('dtype of d is',d.dtype)\n",
    "print('id of d is',d.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64) torch.float64\n",
      "[-1.  2.  3.] float64\n"
     ]
    }
   ],
   "source": [
    "# 使用 torch.from_numpy()初始化\n",
    "a = np.array([1.0, 2, 3])\n",
    "t = torch.from_numpy(a)\n",
    "print(t,t.dtype) #会继承dtype,而非使用默认的torch.int64\n",
    "t[0] = -1\n",
    "print(a,a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32) torch.int32\n",
      "[-1  2  3] int32\n",
      "tensor([1, 2, 3], device='cuda:0', dtype=torch.int32) torch.int32\n",
      "[1 2 3] int32\n"
     ]
    }
   ],
   "source": [
    "# 使用 torch.as_tensor()初始化\n",
    "a = np.array([1, 2, 3])\n",
    "t = torch.as_tensor(a) # 未发生copy\n",
    "print(t,t.dtype)\n",
    "t[0] = -1\n",
    "print(a,a.dtype)\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "t = torch.as_tensor(a, device=torch.device('cuda')) # 发生了copy\n",
    "print(t,t.dtype)\n",
    "t[0] = -1\n",
    "print(a,a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   inf, 6.5000]) torch.float32\n",
      "[1e+46, 6.5] <class 'list'>\n",
      "tensor([-1.0000,  6.5000]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 使用 torch.as_tensor()初始化, but for list\n",
    "a = [9999999999999999999999999999999999999999999999.99999,6.5]\n",
    "t = torch.as_tensor(a) # 发生copy\n",
    "# t = torch.as_tensor(a,dtype=torch.float64) # 发生copy\n",
    "print(t,t.dtype)\n",
    "t[0] = -1\n",
    "print(a,type(a))\n",
    "print(t,t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+46, 6.5000e+00], dtype=torch.float64) torch.float64\n",
      "[1.0e+46 6.5e+00] float64\n"
     ]
    }
   ],
   "source": [
    "# 使用 torch.as_tensor()初始化, but for ndarray\n",
    "a = np.array([9999999999999999999999999999999999999999999999.99999,6.5])\n",
    "t = torch.as_tensor(a) \n",
    "print(t,t.dtype)\n",
    "print(a,a.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "id of b is 3232800067264\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "id of c is 3232800067264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.as_tensor() but for tensor\n",
    "b = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "print(b)\n",
    "print('id of b is',b.data_ptr(),end='\\n\\n')\n",
    "c = torch.as_tensor(b) \n",
    "# c = torch.as_tensor(b,dtype=torch.int64) \n",
    "print(c)\n",
    "print('id of c is',c.data_ptr(),end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], dtype=torch.float64, requires_grad=True)\n",
      "id of a is 3232796615168\n",
      "\n",
      "True False\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], dtype=torch.float64)\n",
      "id of b is 3232796612608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.clone() 可微\n",
    "a = torch.tensor(np.array([[1.0, 2, 3], [4, 5, 6]]), requires_grad=True)\n",
    "print(a)\n",
    "print('id of a is',a.storage().data_ptr(),end='\\n\\n')\n",
    "\n",
    "# b = torch.tensor(a)\n",
    "# b = torch.clone(a)\n",
    "b = torch.clone(a).detach()\n",
    "print(a.requires_grad, b.requires_grad)\n",
    "\n",
    "print(b)\n",
    "print('id of b is',b.storage().data_ptr(),end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232495737088 3232495737152\n",
      "True True\n",
      "tensor(2.)\n",
      "None\n",
      "tensor(5.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\anaconda\\envs\\gpt\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# 情况一\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = a.clone()\n",
    "print(a.data_ptr(), b.data_ptr())\n",
    "# (2892334894104, 2892334859464)  # 表明a和b不是同一个对象\n",
    "print(a.requires_grad, b.requires_grad)\n",
    "# (True, True)  # 两者的requires_grad都是True\n",
    "c = a * 2\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "# tensor(2.)\n",
    "\n",
    "d = b * 3\n",
    "d.backward()\n",
    "print(b.grad) # b的梯度值为None\n",
    "\n",
    "print(a.grad)\n",
    "# tensor(5.)  # b的梯度叠加在a上\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232495738304 3232495737088\n",
      "False False\n",
      "tensor(3.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 情况二：\n",
    "a = torch.tensor(1.0, requires_grad=False)\n",
    "b = a.clone()\n",
    "print(a.data_ptr(), b.data_ptr())\n",
    "print(a.requires_grad, b.requires_grad)\n",
    "# (False False)  # 两者的requires_grad都是True\n",
    "b.requires_grad_()\n",
    "\n",
    "\n",
    "d = b * 3\n",
    "d.backward()\n",
    "print(b.grad) # b的梯度值为tensor(3.)\n",
    "print(a.grad) # None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232495738304 3232495739392\n",
      "True False\n",
      "tensor(2.)\n",
      "tensor(3.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# 采用detach()的效果 ## 被采用标记\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = a.clone().detach() # 割裂a和b为两个网络\n",
    "print(a.data_ptr(), b.data_ptr())\n",
    "print(a.requires_grad, b.requires_grad)\n",
    "# (True False)  # 两者的requires_grad都是True\n",
    "b.requires_grad_()\n",
    "\n",
    "c = a * 2\n",
    "c.backward()\n",
    "print(a.grad)# a的梯度值为tensor(2.)\n",
    "\n",
    "d = b * 3\n",
    "d.backward()\n",
    "print(b.grad) # b的梯度值为tensor(3.)\n",
    "\n",
    "print(a.grad) # a的梯度值为tensor(2.)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b87ae1c415bf879c31e2b7111dedfdde99e7fb6020d8739c335659c250c99311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
