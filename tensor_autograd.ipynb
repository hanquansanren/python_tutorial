{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0.weight True\n",
      "0.bias True\n",
      "1.weight True\n",
      "1.bias True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(8, 3, 50, 100)\n",
    "print(input.requires_grad)\n",
    "# False\n",
    "\n",
    "net = nn.Sequential(nn.Conv2d(3, 16, 3, 1),\n",
    "                    nn.Conv2d(16, 32, 3, 1))\n",
    "for param in net.named_parameters():\n",
    "    print(param[0], param[1].requires_grad)\n",
    "# 0.weight True\n",
    "# 0.bias True\n",
    "# 1.weight True\n",
    "# 1.bias True\n",
    "\n",
    "output = net(input)\n",
    "print(output.requires_grad)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "torch.Size([16, 3, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([32, 16, 3, 3])\n",
      "torch.Size([32])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(8, 3, 50, 100)\n",
    "print(input.requires_grad)\n",
    "# False\n",
    "\n",
    "net = nn.Sequential(nn.Conv2d(3, 16, 3, 1),\n",
    "                    nn.Conv2d(16, 32, 3, 1))\n",
    "\n",
    "                    \n",
    "for param in net.named_parameters():\n",
    "    param[1].requires_grad = False\n",
    "    print(param[0], param[1].requires_grad)\n",
    "# 0.weight False\n",
    "# 0.bias False\n",
    "# 1.weight False\n",
    "# 1.bias False\n",
    "\n",
    "\n",
    "# for param in net.parameters():\n",
    "#     print(param.size())\n",
    "\n",
    "\n",
    "\n",
    "output = net(input)\n",
    "print(output.requires_grad)\n",
    "# False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, padding=1, bias=False)\n",
    "        self.linear = nn.Linear(32*10*10, 20, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.linear(x.view(x.size(0), -1))\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# 创建一个很简单的网络：两个卷积层，一个全连接层\n",
    "model = Simple()\n",
    "# 为了方便观察数据变化，把所有网络参数都初始化为 0.1\n",
    "for m in model.parameters():\n",
    "    m.data.fill_(0.1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "model.train()\n",
    "# 模拟输入8个 sample，每个的大小是 10x10，\n",
    "# 值都初始化为1，让每次输出结果都固定，方便观察\n",
    "images = torch.ones(8, 3, 10, 10)\n",
    "targets = torch.ones(8, dtype=torch.long)\n",
    "\n",
    "output = model(images)\n",
    "print(output.shape)\n",
    "# torch.Size([8, 20])\n",
    "\n",
    "loss = criterion(output, targets)\n",
    "\n",
    "print(model.conv1.weight.grad)\n",
    "# None\n",
    "loss.backward()\n",
    "print(model.conv1.weight.grad[0][0][0])\n",
    "# tensor([-0.0782, -0.0842, -0.0782])\n",
    "# 通过一次反向传播，计算出网络参数的导数，\n",
    "# 因为篇幅原因，我们只观察一小部分结果\n",
    "\n",
    "print(model.conv1.weight[0][0][0])\n",
    "# tensor([0.1000, 0.1000, 0.1000], grad_fn=<SelectBackward>)\n",
    "# 我们知道网络参数的值一开始都初始化为 0.1 的\n",
    "\n",
    "optimizer.step()\n",
    "print(model.conv1.weight[0][0][0])\n",
    "# tensor([0.1782, 0.1842, 0.1782], grad_fn=<SelectBackward>)\n",
    "# 回想刚才我们设置 learning rate 为 1，这样，\n",
    "# 更新后的结果，正好是 (原始权重 - 求导结果) ！\n",
    "\n",
    "optimizer.zero_grad()\n",
    "print(model.conv1.weight.grad[0][0][0])\n",
    "# tensor([0., 0., 0.])\n",
    "# 每次更新完权重之后，我们记得要把导数清零啊，\n",
    "# 不然下次会得到一个和上次计算一起累加的结果。\n",
    "# 当然，zero_grad() 的位置，可以放到前边去，\n",
    "# 只要保证在计算导数前，参数的导数是清零的就好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9., 2., 2.], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0.], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([7., 0, 0], requires_grad=True)\n",
    "b = a + 2\n",
    "print(b)\n",
    "# tensor([9., 2., 2.], grad_fn=<AddBackward0>)\n",
    "\n",
    "loss = torch.mean(b * b)\n",
    "\n",
    "b_ = b.data\n",
    "b_.zero_()\n",
    "print(b)\n",
    "# tensor([0., 0., 0.], grad_fn=<AddBackward0>)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(a.grad)\n",
    "# tensor([0., 0., 0.])\n",
    "\n",
    "# 其实正确的结果应该是：\n",
    "# tensor([6.0000, 1.3333, 1.3333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([7., 0, 0], requires_grad=True)\n",
    "b = a + 2\n",
    "print(b)\n",
    "# tensor([9., 2., 2.], grad_fn=<AddBackward0>)\n",
    "\n",
    "loss = torch.mean(b * b)\n",
    "\n",
    "b_ = b.detach()\n",
    "b_.zero_()\n",
    "print(b)\n",
    "# tensor([0., 0., 0.], grad_fn=<AddBackward0>)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(a.grad)\n",
    "# tensor([0., 0., 0.])\n",
    "\n",
    "# 其实正确的结果应该是：\n",
    "# tensor([6.0000, 1.3333, 1.3333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "class net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net1, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10) for i in range(2)])\n",
    "    def forward(self, x):\n",
    "        for m in self.linears:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "net = net1()\n",
    "print(net)\n",
    "# net1(\n",
    "#   (modules): ModuleList(\n",
    "#     (0): Linear(in_features=10, out_features=10, bias=True)\n",
    "#     (1): Linear(in_features=10, out_features=10, bias=True)\n",
    "#   )\n",
    "# )\n",
    "\n",
    "for param in net.parameters():\n",
    "    print(type(param.data), param.size())\n",
    "# <class 'torch.Tensor'> torch.Size([10, 10])\n",
    "# <class 'torch.Tensor'> torch.Size([10])\n",
    "# <class 'torch.Tensor'> torch.Size([10, 10])\n",
    "# <class 'torch.Tensor'> torch.Size([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss grad:  tensor(1.)\n",
      "l4 grad:  tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "l1 grad:  tensor([[7., 7.],\n",
      "        [7., 7.]])\n"
     ]
    }
   ],
   "source": [
    "# 钩子的用法\n",
    "input = torch.ones([2, 2], requires_grad=False)\n",
    "w1 = torch.tensor(2.0, requires_grad=True)\n",
    "w2 = torch.tensor(3.0, requires_grad=True)\n",
    "w3 = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "l1 = input*w1\n",
    "l2 = l1 + w2\n",
    "l3 = l1 * w3\n",
    "l4 = l2 * l3\n",
    "\n",
    "loss = l4.mean()\n",
    "\n",
    "l1.register_hook(lambda grad: print('l1 grad: ', grad))\n",
    "l4.register_hook(lambda grad: print('l4 grad: ', grad))\n",
    "loss.register_hook(lambda grad: print('loss grad: ', grad))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# loss grad:  tensor(1.)\n",
    "# l4 grad:  tensor([[0.2500, 0.2500],\n",
    "#         [0.2500, 0.2500]])\n",
    "# l1 grad:  tensor([[7., 7.],\n",
    "#         [7., 7.]])\n",
    "\n",
    "# print(loss.grad)\n",
    "# None\n",
    "# loss 的 grad 在 print 完之后就被清除掉了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2044],\n",
      "        [0.9519]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "grad_list = []\n",
    "\n",
    "def print_grad(grad):\n",
    "    # grad=5\n",
    "    grad_list.append(grad)\n",
    "    print(grad)\n",
    "\n",
    "\n",
    "\n",
    "x = Variable(torch.randn(2, 1), requires_grad=True)\n",
    "y = x+2\n",
    "z = torch.mean(torch.pow(y, 2))\n",
    "lr = 1e-3\n",
    "y.register_hook(print_grad)\n",
    "\n",
    "\n",
    "z.backward()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b87ae1c415bf879c31e2b7111dedfdde99e7fb6020d8739c335659c250c99311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
